{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuaderno de presentación prueba NLP Davivienda\n",
    "\n",
    "En este cuaderno se hace una breve descripción de aspectos relevantes obtenidos en los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download es_core_news_sm\n",
    "#!pip install transformers\n",
    "#!pip install tensorflow\n",
    "#!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "nlp=spacy.load(\"es_core_news_sm\")\n",
    "from spacy import displacy\n",
    "\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserScreenName</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Tweet URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Andrés Langebaek</td>\n",
       "      <td>@ALangebaek</td>\n",
       "      <td>2021-12-01T20:43:12.000Z</td>\n",
       "      <td>Andrés Langebaek\\n@ALangebaek\\n·\\n1 dic.</td>\n",
       "      <td>La confianza se afectó. El indicador de confia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>['https://pbs.twimg.com/media/FFjL57eXMAISBnk?...</td>\n",
       "      <td>https://twitter.com/ALangebaek/status/14661458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Plaza Futura</td>\n",
       "      <td>@plaza_futura</td>\n",
       "      <td>2021-12-01T21:18:10.000Z</td>\n",
       "      <td>Plaza Futura\\n@plaza_futura\\n·\\n1 dic.</td>\n",
       "      <td>Buscamos la accesibilidad y mejor atención en ...</td>\n",
       "      <td>✅ ✅ ✅ ✅ ✅</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['https://pbs.twimg.com/ext_tw_video_thumb/146...</td>\n",
       "      <td>https://twitter.com/plaza_futura/status/146615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Julián Martinez</td>\n",
       "      <td>@JulianM998</td>\n",
       "      <td>2021-12-01T22:49:11.000Z</td>\n",
       "      <td>Julián Martinez\\n@JulianM998\\n·\\n1 dic.</td>\n",
       "      <td>Señores \\n@Davivienda\\n no he podido ingresar ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/JulianM998/status/14661775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ferchis.</td>\n",
       "      <td>@fergomezr28</td>\n",
       "      <td>2021-12-01T12:29:07.000Z</td>\n",
       "      <td>Ferchis.\\n@fergomezr28\\n·\\n1 dic.</td>\n",
       "      <td>Llevo toda una semana sufriendo intento de hur...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/fergomezr28/status/1466021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MirandaL2</td>\n",
       "      <td>@MirandaSuspLo</td>\n",
       "      <td>2021-12-01T20:52:36.000Z</td>\n",
       "      <td>MirandaL2\\n@MirandaSuspLo\\n·\\n1 dic.</td>\n",
       "      <td>Hemos retrocedido tanto en este país con este ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/MirandaSuspLo/status/14661...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    UserScreenName        UserName                 Timestamp  \\\n",
       "0           0  Andrés Langebaek     @ALangebaek  2021-12-01T20:43:12.000Z   \n",
       "1           1      Plaza Futura   @plaza_futura  2021-12-01T21:18:10.000Z   \n",
       "2           2   Julián Martinez     @JulianM998  2021-12-01T22:49:11.000Z   \n",
       "3           3          Ferchis.    @fergomezr28  2021-12-01T12:29:07.000Z   \n",
       "4           4         MirandaL2  @MirandaSuspLo  2021-12-01T20:52:36.000Z   \n",
       "\n",
       "                                       Text  \\\n",
       "0  Andrés Langebaek\\n@ALangebaek\\n·\\n1 dic.   \n",
       "1    Plaza Futura\\n@plaza_futura\\n·\\n1 dic.   \n",
       "2   Julián Martinez\\n@JulianM998\\n·\\n1 dic.   \n",
       "3         Ferchis.\\n@fergomezr28\\n·\\n1 dic.   \n",
       "4      MirandaL2\\n@MirandaSuspLo\\n·\\n1 dic.   \n",
       "\n",
       "                                       Embedded_text     Emojis  Comments  \\\n",
       "0  La confianza se afectó. El indicador de confia...        NaN       1.0   \n",
       "1  Buscamos la accesibilidad y mejor atención en ...  ✅ ✅ ✅ ✅ ✅       NaN   \n",
       "2  Señores \\n@Davivienda\\n no he podido ingresar ...        NaN       1.0   \n",
       "3  Llevo toda una semana sufriendo intento de hur...        NaN       2.0   \n",
       "4  Hemos retrocedido tanto en este país con este ...        NaN       3.0   \n",
       "\n",
       "  Likes Retweets                                         Image link  \\\n",
       "0     7       19  ['https://pbs.twimg.com/media/FFjL57eXMAISBnk?...   \n",
       "1   NaN      NaN  ['https://pbs.twimg.com/ext_tw_video_thumb/146...   \n",
       "2   NaN        1                                                 []   \n",
       "3     1        2                                                 []   \n",
       "4   NaN        8                                                 []   \n",
       "\n",
       "                                           Tweet URL  \n",
       "0  https://twitter.com/ALangebaek/status/14661458...  \n",
       "1  https://twitter.com/plaza_futura/status/146615...  \n",
       "2  https://twitter.com/JulianM998/status/14661775...  \n",
       "3  https://twitter.com/fergomezr28/status/1466021...  \n",
       "4  https://twitter.com/MirandaSuspLo/status/14661...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lectura de datos\n",
    "\n",
    "df_tweets = pd.read_csv(\"../Datos/davivienda_tweets.csv\")\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza de datos\n",
    "## Iniciamos extrayendo caracteres especiales (\\n)\n",
    "\n",
    "df_tweets['Embedded_text_1'] = df_tweets['Embedded_text'].apply(lambda x: re.sub(r'\\n[ 0-9]*', ' ', x))\n",
    "\n",
    "## Luego extraemos los links\n",
    "df_tweets['links'] = df_tweets['Embedded_text_1'].apply(lambda x: re.findall(r'http\\S+', x))\n",
    "links_title=[]\n",
    "for i in df_tweets['links']:\n",
    "    if len(i)==0:\n",
    "        links_title.append(\"\")\n",
    "    else:\n",
    "        lista_i=[]\n",
    "        for k in i:\n",
    "            #print(k)\n",
    "            try:\n",
    "                html_page = urllib.request.urlopen(k)\n",
    "                soup = BeautifulSoup(html_page)\n",
    "                lista_i.append(soup.title.string)\n",
    "            except:\n",
    "               new_k=k.replace(\"https://\",\"\")\n",
    "               new_k=new_k.replace(\"http://\",\"\")\n",
    "               new_k=re.sub(r'\\.[A-Za-z\\.]*', '', new_k)\n",
    "               new_k=re.sub(r'\\/.*', '', new_k)\n",
    "               lista_i.append(new_k)\n",
    "        links_title.append(lista_i)\n",
    "\n",
    "df_tweets['links_title']=links_title\n",
    "df_tweets['conteo_links'] = df_tweets['links'].apply(lambda x: len(x))\n",
    "df_tweets['Embedded_text_1'] = df_tweets['Embedded_text_1'].apply(lambda x: re.sub(r'http\\S+', 'Link_aqui', x))\n",
    "\n",
    "## Luego extraemos los hashtags\n",
    "df_tweets['hashtags'] = df_tweets['Embedded_text_1'].apply(lambda x: re.findall(r'#\\S+', x))\n",
    "df_tweets['conteo_hashtags'] = df_tweets['hashtags'].apply(lambda x: len(x))\n",
    "\n",
    "## Luego extraemos los menciones\n",
    "df_tweets['menciones'] = df_tweets['Embedded_text_1'].apply(lambda x: re.findall(r'@\\S+', x))\n",
    "df_tweets['conteo_menciones'] = df_tweets['menciones'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Luego extraemos los emojis\n",
    "df_tweets['emojis'] = df_tweets['Embedded_text_1'].apply(lambda x: re.findall(r'\\\\u\\S+', x))\n",
    "df_tweets['conteo_emojis'] = df_tweets['emojis'].apply(lambda x: len(x))\n",
    "\n",
    "## Luego extraemos los RT\n",
    "df_tweets['RT'] = df_tweets['Embedded_text_1'].apply(lambda x: re.findall(r'RT', x))\n",
    "df_tweets['conteo_RT'] = df_tweets['RT'].apply(lambda x: len(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1a8b714d334dbd8be023c3306f7ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e812d99b75af4b0bac86e4629f1a81ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 07:41:45.916833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318bfafc0f204f8a85a4a3503e5404fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fc2eed397c42ba9e77f9bc8b54c10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edbd1ded0ce48e2b5add5c15e6f022f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Análisis de sentimientos\n",
    "\n",
    "## Cargamos el modelo de sentimientos\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp_sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "## Aplicamos el modelo a los tweets\n",
    "\n",
    "df_tweets['sentiment'] = df_tweets['Embedded_text_1'].apply(lambda x: nlp_sentiment(x)[0]['label'])\n",
    "df_tweets['score'] = df_tweets['Embedded_text_1'].apply(lambda x: nlp_sentiment(x)[0]['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 star     1250\n",
       "5 stars     314\n",
       "4 stars     152\n",
       "3 stars      51\n",
       "2 stars      44\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1811.000000\n",
       "mean        0.546778\n",
       "std         0.188656\n",
       "min         0.219787\n",
       "25%         0.389547\n",
       "50%         0.513138\n",
       "75%         0.697739\n",
       "max         0.971901\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3639947a4ddd4a1abd8f1c3fa9caea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d374127afb4b9181ea3bedf5e62598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/434M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dslim/bert-base-NER were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dslim/bert-base-NER.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf549fbbbc94582b953c18a26d5eb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc44be34ec444a5aad1ebdc33bc8024c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac2c0480311432ea65898e7b66c292c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ece05e6b794e7fbf0968cc134041d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Análisis de entidades\n",
    "\n",
    "## Cargamos el modelo de entidades\n",
    "\n",
    "nlp_entidades = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "\n",
    "## Aplicamos el modelo a los tweets\n",
    "\n",
    "df_tweets['entidades'] = df_tweets['Embedded_text_1'].apply(lambda x: nlp_entidades(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFXLMRobertaForSequenceClassification were initialized from the model checkpoint at joeddav/xlm-roberta-large-xnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## Análisis de temas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m## Cargamos el modelo de temas\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m nlp_temas \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mzero-shot-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjoeddav/xlm-roberta-large-xnli\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/pipelines/__init__.py:801\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m             tokenizer_identifier \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    799\u001b[0m             tokenizer_kwargs \u001b[39m=\u001b[39m model_kwargs\n\u001b[0;32m--> 801\u001b[0m         tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    802\u001b[0m             tokenizer_identifier, use_fast\u001b[39m=\u001b[39;49muse_fast, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_kwargs\n\u001b[1;32m    803\u001b[0m         )\n\u001b[1;32m    805\u001b[0m \u001b[39mif\u001b[39;00m load_feature_extractor:\n\u001b[1;32m    806\u001b[0m     \u001b[39m# Try to infer feature extractor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39mif\u001b[39;00m feature_extractor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:637\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    638\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1777\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1775\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1777\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1778\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1779\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1780\u001b[0m     init_configuration,\n\u001b[1;32m   1781\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1782\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1783\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1784\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1785\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1786\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1787\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1932\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1933\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1934\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1935\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1936\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m     \u001b[39m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[0;32m--> 155\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    156\u001b[0m         vocab_file,\n\u001b[1;32m    157\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    158\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    159\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    160\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[1;32m    161\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[1;32m    162\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    163\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    164\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    165\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "## Análisis de temas\n",
    "\n",
    "## Cargamos el modelo de temas\n",
    "\n",
    "nlp_temas = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56c061c6a3605692dd8f68d833a6cd95e25f484a98386b6702ce308f04d6b922"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
